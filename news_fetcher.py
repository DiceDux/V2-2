import requests
from data.data_manager import insert_news, get_connection
from datetime import datetime, timedelta
import re
from ai.news_sentiment_ai import analyze_sentiment
import pandas as pd
import os
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import logging

SYMBOLS = {
    "SOL": ["solana", "SOL", "Sol"],
    "BTC": ["bitcoin", "BTC", "Btc"],
    "ETH": ["ethereum", "ETH", "Eth"],
    "DOGE": ["dogecoin", "DOGE", "Doge"]
}

# ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯ÛŒÙ†Ú¯
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

API_KEY = os.getenv("CRYPTOPANIC_API_KEY")
if not API_KEY:
    logger.error("Ú©Ù„ÛŒØ¯ API CryptoPanic Ø¯Ø± Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
    exit(1)

# Ø´Ù…Ø§Ø±Ø´Ú¯Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§
error_count = 0
MAX_ERROR_LOGS = 5

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((requests.exceptions.RequestException,)),
    reraise=True
)
def fetch_news_from_api(url):
    """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø§Ø² API Ø¨Ø§ Ù…Ú©Ø§Ù†ÛŒØ²Ù… retry"""
    response = requests.get(url)
    response.raise_for_status()
    return response.json()

def check_duplicate_news(symbol, title, published_at):
    """Ú†Ú© Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø®Ø¨Ø± Ù‚Ø¨Ù„Ø§Ù‹ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±"""
    conn = None
    try:
        conn = get_connection()
        if not conn:
            return False
        query = """
            SELECT COUNT(*) FROM news
            WHERE symbol = %s AND title = %s AND published_at = %s
        """
        df = pd.read_sql(query, con=conn, params=(symbol, title, published_at))
        return df.iloc[0, 0] > 0
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ú†Ú© Ú©Ø±Ø¯Ù† Ø®Ø¨Ø± ØªÚ©Ø±Ø§Ø±ÛŒ: {e}")
        return False
    finally:
        if conn:
            conn.close()

def fetch_news():
    global error_count
    logger.info("ğŸ“° Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø±...")
    # ÙÙ‚Ø· Ø§Ø®Ø¨Ø§Ø± Û· Ø±ÙˆØ² Ø§Ø®ÛŒØ± Ø±Ùˆ Ø¨Ú¯ÛŒØ±
    since_date = (datetime.utcnow() - timedelta(days=7)).strftime('%Y-%m-%d')
    url = f"https://cryptopanic.com/api/v1/posts/?auth_token={API_KEY}&public=true&kind=news&published_since={since_date}"

    try:
        data = fetch_news_from_api(url)
        logger.info(f"Ø§Ø®Ø¨Ø§Ø± Ø¯Ø±ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡: {len(data.get('results', []))} Ø®Ø¨Ø±")
        
        if "results" not in data:
            logger.warning("âš ï¸ Ù‡ÛŒÚ† Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            return

        for item in data["results"]:
            title = item.get("title", "")
            content = item.get("content", title)
            if not title:
                continue

            source = item.get("domain", "unknown")
            published_at = item.get("published_at", datetime.utcnow().isoformat())
            # ØªØ¨Ø¯ÛŒÙ„ published_at Ø¨Ù‡ ÙØ±Ù…Øª datetime
            published_at_dt = datetime.strptime(published_at, '%Y-%m-%dT%H:%M:%S%z')
            published_at = published_at_dt.strftime('%Y-%m-%d %H:%M:%S')

            # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± ØªÙ…Ø§Ù… Ù†Ù…Ø§Ø¯Ù‡Ø§
            for symbol, symbol_variants in SYMBOLS.items():
                base_symbol = symbol
                pattern = r'\b(' + '|'.join(re.escape(v) for v in symbol_variants) + r')\b'
                if re.search(pattern, title.upper()) or re.search(pattern, content.upper()):
                    if check_duplicate_news(base_symbol, title, published_at):
                        logger.info(f"âš ï¸ Ø®Ø¨Ø± ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ {base_symbol} - {title[:40]}...")
                        continue

                    # Ù…Ø­Ø§Ø³Ø¨Ù‡ sentiment_score
                    sentiment_score = 0.0
                    if content and content.strip():
                        sentiment_score = analyze_sentiment(content)
                    else:
                        if error_count < MAX_ERROR_LOGS:
                            logger.warning(f"âš ï¸ Ù…Ø­ØªÙˆØ§ ÛŒØ§ Ø¹Ù†ÙˆØ§Ù† Ø®Ø§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø®Ø¨Ø±: {title[:40]}...")
                            error_count += 1
                        elif error_count == MAX_ERROR_LOGS:
                            logger.warning("Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ Ø±Ø³ÛŒØ¯Ù‡ Ø§Ø³Øª. Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ Ù…Ø­Ø¯ÙˆØ¯ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.")
                            error_count += 1

                    # Ø°Ø®ÛŒØ±Ù‡ Ø®Ø¨Ø±
                    insert_news(base_symbol, title, source, published_at, content, sentiment_score)

                    # ÙÛŒÙ„ØªØ± Ø§Ø®Ø¨Ø§Ø± Ù…Ù‡Ù…
                    important_keywords = ['partnership', 'regulation', 'adoption', 'upgrade', 'halving']
                    is_important = any(keyword in title.lower() for keyword in important_keywords)
                    logger.info(f"âœ… Ø®Ø¨Ø± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¨Ø±Ø§ÛŒ {base_symbol} - {title[:40]}... {'(Ù…Ù‡Ù…)' if is_important else ''}")

    except requests.exceptions.RequestException as e:
        if error_count < MAX_ERROR_LOGS:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø±: {e}")
            error_count += 1
        elif error_count == MAX_ERROR_LOGS:
            logger.error("Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ Ø±Ø³ÛŒØ¯Ù‡ Ø§Ø³Øª. Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ Ù…Ø­Ø¯ÙˆØ¯ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.")
            error_count += 1
    except Exception as e:
        if error_count < MAX_ERROR_LOGS:
            logger.error(f"âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø¯Ø± fetch_news: {e}")
            error_count += 1
        elif error_count == MAX_ERROR_LOGS:
            logger.error("Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ Ø±Ø³ÛŒØ¯Ù‡ Ø§Ø³Øª. Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ Ù…Ø­Ø¯ÙˆØ¯ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.")
            error_count += 1

if __name__ == "__main__":
    fetch_news()