import time
import requests
import pandas as pd
from datetime import datetime, timedelta
import logging
from data.data_manager import get_connection
from sqlalchemy import text
import argparse
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯ÛŒÙ†Ú¯
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

INDEXES = {
    "BTC.D": "https://api.coingecko.com/api/v3/global",
    "USDT.D": "https://api.alternative.me/fng/",
    "SPX": "SPY",
    "DXY": "UUP"
}

# Ú©Ù„ÛŒØ¯ API Ø¨Ø±Ø§ÛŒ Alpha Vantage
ALPHA_VANTAGE_API_KEY = "XVJ5HCY0XJO1OR5Q"  # Ú©Ù„ÛŒØ¯ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯

def create_session():
    """Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ù„Ø³Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø§ ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯"""
    session = requests.Session()
    retries = Retry(total=5, backoff_factor=2, status_forcelist=[429, 500, 502, 503, 504])
    session.mount('https://', HTTPAdapter(max_retries=retries))
    return session

def fetch_btc_dominance():
    """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ Bitcoin Dominance Ø§Ø² Coingecko"""
    try:
        session = create_session()
        r = session.get(INDEXES["BTC.D"], timeout=10)
        r.raise_for_status()
        data = r.json()
        btc_dominance = data["data"]["market_cap_percentage"]["btc"]
        return float(btc_dominance)
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª BTC.D Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ: {e}")
        return None

def fetch_historical_btc_dominance(start_date, end_date):
    """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Bitcoin Dominance (ØªÙ‚Ø±ÛŒØ¨ÛŒ)"""
    try:
        btc_dominance = fetch_btc_dominance()
        if btc_dominance is None:
            logger.warning("Ø¯Ø§Ø¯Ù‡ BTC.D Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.")
            return []
        historical_data = []
        current_date = datetime.utcfromtimestamp(start_date)
        end = datetime.utcfromtimestamp(end_date)
        while current_date < end:
            timestamp = int(current_date.timestamp())
            created_at = current_date.strftime('%Y-%m-%d %H:%M:%S')
            historical_data.append((timestamp, btc_dominance, created_at))
            current_date += timedelta(days=1)
        logger.warning("âš ï¸ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ BTC.D ØªÙ‚Ø±ÛŒØ¨ÛŒ Ù‡Ø³ØªÙ†Ø¯ (Ù…Ù‚Ø¯Ø§Ø± Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ù„ Ø¨Ø§Ø²Ù‡).")
        return historical_data
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ BTC.D: {e}")
        return []

def fetch_usdt_dominance():
    """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ Fear & Greed Index Ø¨Ø±Ø§ÛŒ USDT.D"""
    try:
        session = create_session()
        r = session.get(INDEXES["USDT.D"], timeout=10)
        r.raise_for_status()
        return float(r.json()["data"][0]["value"])
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª USDT.D: {e}")
        return None

def fetch_historical_usdt_dominance(start_date, end_date):
    """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Fear & Greed Index"""
    try:
        session = create_session()
        days = (datetime.utcfromtimestamp(end_date) - datetime.utcfromtimestamp(start_date)).days
        url = f"https://api.alternative.me/fng/?limit={days}"
        r = session.get(url, timeout=10)
        r.raise_for_status()
        historical_data = r.json()["data"]
        data = []
        for entry in historical_data:
            value = float(entry["value"])
            timestamp = int(entry["timestamp"])
            if start_date <= timestamp <= end_date:
                created_at = datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
                data.append((timestamp, value, created_at))
        return sorted(data, key=lambda x: x[0])
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ USDT.D: {e}")
        return []

def fetch_alpha_vantage_index(symbol, start_date, end_date):
    """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø§Ø² Alpha Vantage Ùˆ Ù¾Ø± Ú©Ø±Ø¯Ù† Ø±ÙˆØ²Ù‡Ø§ÛŒ ØºÛŒØ±Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ"""
    try:
        session = create_session()
        start = datetime.strptime(start_date, '%Y-%m-%d')
        end = datetime.strptime(end_date, '%Y-%m-%d')

        logger.info(f"Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ {symbol} Ø§Ø² {start_date} ØªØ§ {end_date}")
        url = f"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&outputsize=full&apikey={ALPHA_VANTAGE_API_KEY}"
        r = session.get(url, timeout=10)
        r.raise_for_status()
        data = r.json()

        if "Time Series (Daily)" not in data:
            logger.error(f"âŒ Ø®Ø·Ø§ÛŒ API Alpha Vantage Ø¨Ø±Ø§ÛŒ {symbol}: {data.get('Error Message', 'Unknown error')}")
            return []

        # ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ DataFrame
        time_series = data["Time Series (Daily)"]
        df = pd.DataFrame([
            {
                'date': datetime.strptime(date_str, '%Y-%m-%d'),
                'value': float(values["4. close"])
            }
            for date_str, values in time_series.items()
        ])
        df = df[(df['date'] >= start) & (df['date'] <= end)]
        df = df.sort_values('date')

        if df.empty:
            logger.warning(f"âš ï¸ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ {symbol} Ø¯Ø± Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ù…Ø´Ø®Øµâ€ŒØ´Ø¯Ù‡ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            return []

        # ØªÙˆÙ„ÛŒØ¯ ØªÙ…Ø§Ù… Ø±ÙˆØ²Ù‡Ø§ÛŒ ØªÙ‚ÙˆÛŒÙ…ÛŒ Ø¯Ø± Ø¨Ø§Ø²Ù‡
        all_dates = pd.date_range(start=start, end=end, freq='D')
        full_df = pd.DataFrame(all_dates, columns=['date'])

        # Ø§Ø¯ØºØ§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        merged_df = pd.merge(full_df, df, on='date', how='left')

        # Ù¾Ø± Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± NaN Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ffill Ùˆ bfill
        if merged_df['value'].isna().all():
            logger.warning(f"âš ï¸ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ {symbol} Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            return []
        
        # Ø§Ø¨ØªØ¯Ø§ Ø¨Ø§ bfill Ù¾Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§ÙˆÙ„ÛŒÙ‡ NaN Ø¨Ø§ Ø§ÙˆÙ„ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± Ù…Ø¹ØªØ¨Ø± Ù¾Ø± Ø´ÙˆÙ†Ø¯
        merged_df['value'] = merged_df['value'].bfill()
        # Ø³Ù¾Ø³ Ø¨Ø§ ffill Ù¾Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡ NaN Ù¾Ø± Ø´ÙˆÙ†Ø¯
        merged_df['value'] = merged_df['value'].ffill()

        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±
        historical_data = []
        for _, row in merged_df.iterrows():
            timestamp = int(row['date'].timestamp())
            value = float(row['value'])  # Ù…Ø·Ù…Ø¦Ù† Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ… Ú©Ù‡ NaN ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
            created_at = row['date'].strftime('%Y-%m-%d %H:%M:%S')
            historical_data.append({
                'timestamp': timestamp,
                'value': value,
                'created_at': created_at
            })
        
        return historical_data
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ {symbol} Ø§Ø² Alpha Vantage: {e}")
        return []

def save_index(index_name, value, timestamp, created_at=None):
    """Ø°Ø®ÛŒØ±Ù‡ ØªÚ© Ø´Ø§Ø®Øµ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
    conn = None
    try:
        if created_at is None:
            created_at = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')
        conn = get_connection()
        if not conn:
            logger.error("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³")
            return
        query = """
            INSERT INTO market_indices (index_name, timestamp, value, created_at)
            VALUES (:index_name, :timestamp, :value, :created_at)
            ON DUPLICATE KEY UPDATE value = VALUES(value)
        """
        conn.execute(
            text(query),
            {
                'index_name': index_name,
                'timestamp': timestamp,
                'value': value,
                'created_at': created_at
            }
        )
        conn.commit()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ {index_name}: {e}")
        if conn:
            conn.rollback()
    finally:
        if conn:
            conn.close()

def save_index_batch(index_name, records):
    """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
    conn = None
    try:
        conn = get_connection()
        if not conn:
            logger.error("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³")
            return
        query = """
            INSERT INTO market_indices (index_name, timestamp, value, created_at)
            VALUES (:index_name, :timestamp, :value, :created_at)
            ON DUPLICATE KEY UPDATE value = VALUES(value)
        """
        batch_size = 1000
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            params = [
                {
                    'index_name': index_name,
                    'timestamp': record['timestamp'],
                    'value': record['value'],
                    'created_at': record['created_at']
                }
                for record in batch
            ]
            conn.execute(text(query), params)
            conn.commit()
        logger.info(f"âœ… Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ {index_name}: {len(records)} Ø±Ú©ÙˆØ±Ø¯")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ {index_name}: {e}")
        if conn:
            conn.rollback()
    finally:
        if conn:
            conn.close()

def check_stored_data():
    """Ø¨Ø±Ø±Ø³ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø´Ø§Ø®Øµ"""
    conn = None
    try:
        conn = get_connection()
        if not conn:
            logger.error("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³")
            return
        query = """
            SELECT index_name, COUNT(*), MIN(created_at), MAX(created_at)
            FROM market_indices
            GROUP BY index_name
        """
        df = pd.read_sql(query, con=conn)
        logger.info("ğŸ“Š ÙˆØ¶Ø¹ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡:")
        for _, row in df.iterrows():
            logger.info(f"Ø´Ø§Ø®Øµ: {row['index_name']}, ØªØ¹Ø¯Ø§Ø¯: {row['COUNT(*)']}, Ø¨Ø§Ø²Ù‡: {row['MIN(created_at)']} ØªØ§ {row['MAX(created_at)']}")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {e}")
    finally:
        if conn:
            conn.close()

def run_initial():
    """Ø­Ø§Ù„Øª initial: Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø§Ø² 2017 ØªØ§ 2025"""
    logger.info("ğŸš€ Ø´Ø±ÙˆØ¹ Ø­Ø§Ù„Øª initial: Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ 2017 ØªØ§ 2025")
    start_date = "2017-01-03"  # ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ø§ÙˆÙ„ÛŒÙ† Ø±ÙˆØ² Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ ØªØºÛŒÛŒØ± Ú©Ø±Ø¯
    end_date = "2025-05-16"
    start_ts = int(datetime.strptime(start_date, '%Y-%m-%d').timestamp())
    end_ts = int(datetime.strptime(end_date, '%Y-%m-%d').timestamp())

    # SPX
    logger.info("Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ SPX...")
    spx_data = fetch_alpha_vantage_index(INDEXES["SPX"], start_date, end_date)
    logger.info(f"ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ SPX: {len(spx_data)}")
    save_index_batch("SPX", spx_data)
    time.sleep(12)  # ØªØ£Ø®ÛŒØ± Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ù†Ø±Ø® Alpha Vantage

    # DXY
    logger.info("Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ DXY...")
    dxy_data = fetch_alpha_vantage_index(INDEXES["DXY"], start_date, end_date)
    logger.info(f"ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ DXY: {len(dxy_data)}")
    save_index_batch("DXY", dxy_data)

    # BTC.D
    logger.info("Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ BTC.D...")
    btc_d_historical = fetch_historical_btc_dominance(start_ts, end_ts)
    logger.info(f"ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ BTC.D: {len(btc_d_historical)}")
    save_index_batch("BTC.D", [{'timestamp': t, 'value': v, 'created_at': c} for t, v, c in btc_d_historical])

    # USDT.D
    logger.info("Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ USDT.D...")
    usdt_d_historical = fetch_historical_usdt_dominance(start_ts, end_ts)
    logger.info(f"ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ USDT.D: {len(usdt_d_historical)}")
    save_index_batch("USDT.D", [{'timestamp': t, 'value': v, 'created_at': c} for t, v, c in usdt_d_historical])

def run_recovery():
    """Ø­Ø§Ù„Øª recovery: Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø§Ø² 2017 ØªØ§ 2025"""
    logger.info("ğŸ”„ Ø´Ø±ÙˆØ¹ Ø­Ø§Ù„Øª recovery: Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ 2017 ØªØ§ 2025")
    run_initial()

def run_live():
    """Ø­Ø§Ù„Øª live: Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…Ø¯Ø§ÙˆÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
    logger.info("ğŸŒ Ø´Ø±ÙˆØ¹ Ø­Ø§Ù„Øª live: Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…Ø¯Ø§ÙˆÙ…")
    while True:
        try:
            timestamp = int(time.time())
            # BTC.D
            btc_d = fetch_btc_dominance()
            if btc_d is not None:
                save_index("BTC.D", btc_d, timestamp)

            # USDT.D
            usdt_d = fetch_usdt_dominance()
            if usdt_d is not None:
                save_index("USDT.D", usdt_d, timestamp)

            # SPX Ùˆ DXY
            today = datetime.utcnow().strftime('%Y-%m-%d')
            yesterday = (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d')
            for index_name, symbol in [("SPX", INDEXES["SPX"]), ("DXY", INDEXES["DXY"])]:
                time.sleep(12)
                data = fetch_alpha_vantage_index(symbol, yesterday, today)
                if data:
                    latest = data[-1]
                    save_index(index_name, latest['value'], latest['timestamp'], latest['created_at'])

            logger.info("â³ Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¨Ø¹Ø¯ÛŒ...")
            time.sleep(86400)  # Ø±ÙˆØ²Ø§Ù†Ù‡
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø­Ø§Ù„Øª live: {e}")
            time.sleep(3600)  # 1 Ø³Ø§Ø¹Øª ØµØ¨Ø±

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Market Index Updater")
    parser.add_argument("--mode", choices=["initial", "recovery", "live"], default="live",
                        help="Ø­Ø§Ù„Øª Ø§Ø¬Ø±Ø§: initial (2017-2025), recovery (2017-2025), live (Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…Ø¯Ø§ÙˆÙ…)")
    args = parser.parse_args()

    if args.mode == "initial":
        run_initial()
    elif args.mode == "recovery":
        run_recovery()
    elif args.mode == "live":
        run_live()

    check_stored_data()
    logger.info("âœ… Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")